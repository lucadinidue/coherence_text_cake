{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d109e5-9a07-49a4-b224-ca091a217020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, default_data_collator, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import logging\n",
    "import torch\n",
    "import ast\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c59252-56f4-455b-9f23-4f3412db400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logging():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "    # console_handler = logging.StreamHandler()\n",
    "    # logger.setLevel(logging.INFO)\n",
    "    # console_handler.setLevel(logging.INFO)\n",
    "    # formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s: %(message)s')\n",
    "    # console_handler.setFormatter(formatter)\n",
    "    # logger.addHandler(console_handler)\n",
    "    # return logger, console_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9318c9d-0f95-4266-bfd5-c26a6d66103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = init_logging() #, console_handler = init_logging()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f'Device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0f708-dc5d-4f2c-ba52-0f09c915a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/deberta-v3-xsmall'\n",
    "data_path = '../data/en.tsv'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62861ec8-017c-4715-93c5-3f271788090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files=data_path, sep='\\t', converters={'sentences': ast.literal_eval})\n",
    "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "logger.info(f\"Dataset loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3e8a8-3df3-47af-855e-717465349e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53e457-ce09-4b2b-b340-14459f38b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    # dataset = dataset['train'].remove_columns('sub_sentences')\n",
    "\n",
    "    def concatenate_sentences(example):\n",
    "        # example['sentences'] = ' '.join(example['sentences'])\n",
    "        example['sentences'] = ' '.join(example[sentence] for sentence in ['sentence_0', 'sentence_1', 'sentence_2', 'sentence_3'])\n",
    "        return example\n",
    "    \n",
    "    dataset = dataset.map(concatenate_sentences, \n",
    "                          desc='Concatenatings passage sentences.')\n",
    "\n",
    "    def preprocessing_function(examples):\n",
    "        result = tokenizer(examples['sentences'], padding='max_length', max_length=256, truncation=True)\n",
    "        result['label'] = [1 if perturb_type is None else 0 for perturb_type in examples['perturbation']]\n",
    "        return result\n",
    "\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "            preprocessing_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c04e43-0016-48ef-b45c-cf1726faddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_dataset(dataset['train'], tokenizer)\n",
    "eval_dataset = preprocess_dataset(dataset['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4aab95-e6ba-45d6-8437-f6a70bbe25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Dataset is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d9ad3-f4f6-487c-8793-6441c4e5b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'prova'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016da55-27ea-4be9-8c4b-78be1590b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    # warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    # weight_decay=cf.weight_decay,               # strength of weight decay\n",
    "    save_strategy=\"no\",\n",
    "    # learning_rate=cf.lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996ca0c-6d90-4ef1-85db-9a11d3c410ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('accuracy')#, cache_dir=training_args.cache_dir)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "    if len(result) > 1:\n",
    "        result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7153f-59cc-47b1-ae36-df32c8cd3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "#     preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    \n",
    "#     # Compute the loss\n",
    "#     loss_fct = CrossEntropyLoss()\n",
    "#     loss = loss_fct(preds.view(-1, preds.shape[-1]), p.label_ids.view(-1))\n",
    "    \n",
    "#     result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "#     result[\"loss\"] = loss.item()\n",
    "    \n",
    "#     if len(result) > 1:\n",
    "#         result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e412b1-973d-47b2-98f7-ab894f08f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b07ac-15d8-4008-9cd5-3ea0df17ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000c84d-af82-4f15-bf30-b4155ac01c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca595a1-29e8-48e3-88de-4064f99a5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcc486-2a60-4f69-baa0-d27481410e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a7b20-279d-469a-b228-3f97a9f58c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
